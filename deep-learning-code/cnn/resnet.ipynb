{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "resnet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cvLiKlk53vQ"
      },
      "source": [
        "# Mạng phần dư (ResNet)\n",
        "<!-- :label:`sec_resnet` -->\n",
        "\n",
        "Khi thiết kế các mạng ngày càng sâu, ta cần hiểu việc thêm các tầng sẽ tăng độ phức tạp và khả năng biểu diễn của mạng như thế nào. Quan trọng hơn là khả năng thiết kế các mạng trong đó việc thêm các tầng vào mạng chắc chắn sẽ làm tăng tính biểu diễn thay vì chỉ tạo ra một chút khác biệt. \n",
        "\n",
        "Để làm được điều này, chúng ta cần một chút lý thuyết.\n",
        "\n",
        "## Function Classes\n",
        "\n",
        "Coi $\\mathcal{F}$ là một lớp các hàm mà một kiến trúc mạng cụ thể(Cùng với tốc độ học và các siêu tham số khác) có thể đạt được.\n",
        "Nói cách khác, với mọi hàm số $f \\in \\mathcal{F}$ luôn tồn tại một số tập tham số (như weights và biases) có thể tìm được bằng việc huấn luyện trên một tập dữ liệu phù hợp.\n",
        "\n",
        "Giả sử $f^*$ là hàm chúng ta cần tìm.\n",
        "Nếu nó thuộc tập $\\mathcal{F}$ thì sẽ tốt, nhưng thương không may mắn như vậy :)) Thay vào đó, Chúng ta sẽ cố gấng tìm các hàm số  $f^*_\\mathcal{F}$  tố nhất có thể trong tập $\\mathcal{F}$.\n",
        "\n",
        "Ví dụ, chúng ta thử giải bài toán tối ưu sau, với tập dataset cho trước, (Features $\\mathbf{X}$\n",
        "và labels $\\mathbf{y}$):\n",
        "\n",
        "$$f^*_\\mathcal{F} \\stackrel{\\mathrm{def}}{=} \\mathop{\\mathrm{argmin}}_f L(\\mathbf{X}, \\mathbf{y}, f) \\text{ subject to } f \\in \\mathcal{F}.$$\n",
        "\n",
        "Khá hợp lý khi giả sử rằng nếu thiết kế một kiến trúc khác $\\mathcal{F}'$ chúng ta có thể đạt được kết quả tốt hơn. Nói cách khác, chúng ta kỳ vọng $f^*_{\\mathcal{F}'}$ sẽ \"tốt hơn\"  $f^*_{\\mathcal{F}}$. Tuy nhiên, nếu $\\mathcal{F} \\not\\subseteq \\mathcal{F}'$ thì không thể khẳng định là $f^*_{\\mathcal{F}'}$ tốt hơn $f^*_{\\mathcal{F}}$. Thực tế, $f^*_{\\mathcal{F}'}$ thậm chí có thể xấu hơn. \n",
        "\n",
        "Việc thêm các tầng không phải lúc nào cũng làm tăng tính biểu diễn của mạng. Xem ví dụ ở hình sau, chúng ta thấy rằng ở hình bên trái(các lớp hàm số tổng quát), \n",
        "$\\mathcal{F}_3$ gần $f^*$ hơn là $\\mathcal{F}_1$, $\\mathcal{F}_6$, không thể đảm bảo rằng tăng độ phực tạp có thể giảm khoảng cách với  $f^*$. Trong khi đó, với  ở hình bên phải(các lớp với hàm số lống nhau)\n",
        "với $\\mathcal{F}_1 \\subseteq \\ldots \\subseteq \\mathcal{F}_6$\n",
        "chúng ta có thể tránh được vấn đề ở trường hợp các lớp hàm số tổng quát, khi mà khoảng cách đến hàm cần tìm  $f^*$, trên thực tế có thể tăng khi độ phức tạp tăng lên\n",
        "\n",
        "![For non-nested function classes, a larger (indicated by area) function class does not guarantee to get closer to the \"truth\" function ($f^*$). This does not happen in nested function classes.](http://d2l.ai/_images/functionclasses.svg)\n",
        "<!-- :label:`fig_functionclasses` -->\n",
        "\n",
        "Chỉ khi các lớp hàm lớn hơn chứa các lớp nhỏ hơn, thì mới đảm bảo rằng việc tăng thêm các tầng sẽ tăng khả năng biểu diễn của mạng.\n",
        "Nếu ta huấn luyện tầng mới được thêm vào thành một ánh xạ đồng nhất $f(\\mathbf{x}) = \\mathbf{x}$, thì mô hình mới sẽ hiệu quả ít nhất bằng mô hình ban đầu. Vì tầng được thêm vào có thể khớp dữ liệu huấn luyện tốt hơn, dẫn đến sai số huấn luyện cũng nhỏ hơn.\n",
        "\n",
        "Đây là câu hỏi mà He và các cộng sự đã suy nghĩ khi nghiên cứu các mô hình thị giác sâu năm 2016. Ý tưởng trọng tâm của ResNet là mỗi tầng được thêm vào nên có một thành phần là hàm số đồng nhất\n",
        "\n",
        "Cách suy nghĩ này khá trừu tượng nhưng lại dẫn đến một lời giải đơn giản khá ngạc nhiên, một khối phần dư (residual block). Với ý tưởng này, ResNet đã chiến thắng cuộc thi Nhận dạng Ảnh ImageNet năm 2015. Thiết kế này có ảnh hưởng sâu sắc tới việc xây dựng các mạng nơ-ron sâu.\n",
        "\n",
        "## (**Residual Blocks**)\n",
        "\n",
        "Chúng ta sẽ tập trung vào mạng sau đây. Đầu vào là $\\mathbf{x}$,\n",
        "hàm ánh xạ cần học được là $f(\\mathbf{x})$, và được dùng làm đầu vào của hàm kích hoạt(on top).\n",
        "Phần nằm trong viền nét đứt bên trái phải khớp trực tiếp với ánh xạ $f(\\mathbf{x})$.\n",
        "Ở hình bên phải, phần trong viền nét đứt cần biểu diễn được *residual mapping* $f(\\mathbf{x}) - \\mathbf{x}$, giống với cái tên của nó :)).\n",
        "\n",
        "Trên thực tế, ánh xạ phần dư thường dễ tối ưu hơn, chúng ta chỉ cần để weights và biases (e.g., fully-connected layer and convolutional layer)\n",
        "nằm trong phần nét đứt\n",
        "về 0. Trong ảnh bên phải, chúng ta thấy có một shortcut connection nối ngay tử input. Với resudual blocks, quá trình lan truyền thuận nhanh hơn thông qua kết nối residual.\n",
        "\n",
        "![A regular block (left) and a residual block (right).](https://github.com/d2l-ai/d2l-tensorflow-colab/blob/master/img/residual-block.svg?raw=1)\n",
        "<!-- :label:`fig_residual_block` -->\n",
        "\n",
        "\n",
        "ResNet có thiết kế tầng tích chập $3\\times 3$ giống VGG. Khối phần dư có hai tầng tích chập  $3\\times 3$  với cùng số kênh đầu ra. Mỗi tầng tích chập được theo sau bởi một tầng chuẩn hóa theo batch và một hàm kích hoạt ReLU. Ta đưa đầu vào qua khối phần dư rồi cộng với chính nó trước hàm kích hoạt ReLU cuối cùng. Thiết kế này đòi hỏi đầu ra của hai tầng tích chập phải có cùng kích thước với đầu vào, để có thể cộng lại với nhau. Nếu muốn thay đổi số lượng kênh hoặc sải bước trong khối phần dư, cần thêm một tầng tích chập  $1\\times 1$  để thay đổi kích thước đầu vào tương ứng ở nhánh ngoài. Hãy cùng xem đoạn mã bên dưới."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebFrwYZo6Jh8"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, in_channels, num_channels, use_1x1conv=False, strides=1):\n",
        "        super(Residual, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels,num_channels, kernel_size=3, padding=1,stride=strides)\n",
        "        self.conv2 = nn.Conv2d(num_channels,num_channels, kernel_size=3, padding=1)\n",
        "        self.conv3 = None # Đừng sửa None này nhé :!\n",
        "        if use_1x1conv:\n",
        "            self.conv3 = nn.Conv2d(in_channels,num_channels,kernel_size=1,stride=strides)\n",
        "        self.bn1 = nn.BatchNorm2d(num_channels)\n",
        "        self.bn2 = nn.BatchNorm2d(num_channels)\n",
        "\n",
        "    def forward(self, X):\n",
        "        Y = nn.ReLU()(self.bn1(self.conv1(X)))\n",
        "        Y = self.bn2(self.conv2(Y))\n",
        "        if self.conv3:\n",
        "            X = self.conv3(X)\n",
        "        return nn.ReLU()(Y + X)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jm-om8nI6qFy"
      },
      "source": [
        "Đoạn mã này tạo ra hai loại mạng: một loại cộng đầu vào vào đầu ra trước khi áp dụng hàm phi tuyến ReLU (khi use_1x1conv=True), còn ở loại thứ hai chúng ta thay đổi số kênh và độ phân giải bằng một tầng tích chập  1×1  trước khi thực hiện phép cộng\n",
        "\n",
        "![ResNet block with and without $1 \\times 1$ convolution.](https://github.com/d2l-ai/d2l-tensorflow-colab/blob/master/img/resnet-block.svg?raw=1)\n",
        "<!-- :label:`fig_resnet_block` -->\n",
        "\n",
        "<!-- Now let us look at [**a situation where the input and output are of the same shape**]. -->\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eamO2Ufh6q8g"
      },
      "source": [
        "X = torch.randn((4, 3, 6, 6)) \n",
        "blk = Residual(in_channels = 3, num_channels = 3)\n",
        "assert blk(X).shape  == (4,3,6,6)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZvuuoAd6z56"
      },
      "source": [
        "Chúng ta cũng có thể giảm một nửa kích thước chiều cao và chiều rộng của đầu ra trong khi tăng số kênh.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcpSGH4U60R8"
      },
      "source": [
        "blk = Residual(in_channels = 3,num_channels = 6, use_1x1conv=True, strides=2)\n",
        "assert blk(X).shape == (4,6,3,3)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYoVX_9T62mp"
      },
      "source": [
        "## [**ResNet Model**]\n",
        "\n",
        "Hai tầng đầu tiên của ResNet giống hai tầng đầu tiên của GoogLeNet: tầng tích chập $7\\times 7$  với 64 kênh đầu ra và sải bước 2, theo sau bởi tầng maxpool  $3\\times 3$ với sải bước 2. Sự khác biệt là trong ResNet, mỗi tầng tích chập theo sau bởi tầng chuẩn hóa theo batch.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kchbj6uf64La"
      },
      "source": [
        "net = nn.Sequential()\n",
        "net.add_module(\"conv\",nn.Conv2d(1,64, kernel_size=7, stride=2, padding=3))\n",
        "net.add_module(\"batchnorm\",nn.BatchNorm2d(64))\n",
        "net.add_module(\"Relu\",nn.ReLU())\n",
        "net.add_module(\"maxpool\",nn.MaxPool2d(3, stride = 2, padding = 1))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2nvUXeV65tS"
      },
      "source": [
        "GoogLeNet sử dụng bốn mô-đun được tạo thành từ các khối Inception. ResNet sử dụng bốn mô-đun được tạo thành từ các khối phần dư có cùng số kênh đầu ra. Mô-đun đầu tiên có số kênh bằng số kênh đầu vào. Vì trước đó đã sử dụng tầng gộp cực đại với sải bước 2, nên không cần phải giảm chiều cao và chiều rộng ở mô-đun này. Trong các mô-đun sau, khối phần dư đầu tiên nhân đôi số kênh, đồng thời giảm một nửa chiều cao và chiều rộng.\n",
        "\n",
        "Bây giờ ta sẽ lập trình mô-đun này. Chú ý rằng mô-đun đầu tiên được xử lý khác một chút."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpKCqzh_66OZ"
      },
      "source": [
        "def resnet_block(in_channels ,num_channels, num_residuals, first_block=False):\n",
        "    blk = nn.Sequential()\n",
        "    for i in range(num_residuals):\n",
        "        if i == 0 and not first_block:\n",
        "            blk.add_module('residual_{}'.format(i),Residual(in_channels , num_channels, use_1x1conv=True, strides=2))\n",
        "        else:\n",
        "            blk.add_module('residual_{}'.format(i),Residual(num_channels, num_channels))\n",
        "    return blk"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5CFog2b69pr"
      },
      "source": [
        "Sau đó, chúng ta thêm các khối phần dư vào ResNet. Ở đây, mỗi mô-đun có hai khối phần dư."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSVzt0lA6-I7"
      },
      "source": [
        "net.add_module('resnet_block1',resnet_block(64, 64, 2, first_block=True))\n",
        "net.add_module('resnet_block2',resnet_block(64,128, 2))\n",
        "net.add_module('resnet_block3',resnet_block(128,256, 2))\n",
        "net.add_module('resnet_block4',resnet_block(256,512, 2))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L00tHToW7Alv"
      },
      "source": [
        "Cuối cùng, giống như GoogLeNet, ta thêm một tầng GlobalAvgPool2D và một tầng Dense."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLX0IgV77CeN"
      },
      "source": [
        "net.add_module('GlobalAvr',nn.AdaptiveAvgPool2d((1, 1)))\n",
        "net.add_module('Flatten',nn.Flatten())\n",
        "net.add_module('FC',nn.Linear(512,10))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1irEBbF7FRX"
      },
      "source": [
        "Có 4 tầng tích chập trong mỗi mô-đun (không tính tầng tích chập  1×1 ). Cộng thêm tầng tích chập đầu tiên và tầng kết nối đầy đủ cuối cùng, mô hình có tổng cộng 18 tầng. Do đó, mô hình này thường được gọi là ResNet-18. Có thể thay đổi số kênh và các khối phần dư trong mô-đun để tạo ra các mô hình ResNet khác nhau, ví dụ mô hình 152 tầng của ResNet-152. Mặc dù có kiến trúc lõi tương tự như GoogLeNet, cấu trúc của ResNet đơn giản và dễ sửa đổi hơn. Tất cả các yếu tố này dẫn đến sự phổ cập nhanh chóng và rộng rãi của ResNet\n",
        "\n",
        "<!-- :numref:`fig_resnet18` depicts the full ResNet-18. -->\n",
        "\n",
        "![The ResNet-18 architecture.](http://d2l.ai/_images/resnet18.svg)\n",
        "<!-- :label:`fig_resnet18` -->\n",
        "\n",
        "Trước khi huấn luyện, hãy quan sát thay đổi của kích thước đầu vào qua các mô-đun khác nhau trong ResNet. Như trong tất cả các kiến trúc trước, độ phân giải giảm trong khi số lượng kênh tăng đến khi tầng gộp trung bình toàn cục tổng hợp tất cả các đặc trưng."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DdQudPL7IwV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e34808d-9bec-4b9a-b659-565c76394af0"
      },
      "source": [
        "X = torch.randn((1, 1, 224, 224))\n",
        "for layer in net:\n",
        "    X = layer(X)\n",
        "    print(layer.__class__.__name__, 'output shape:\\t', X.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conv2d output shape:\t torch.Size([1, 64, 112, 112])\n",
            "BatchNorm2d output shape:\t torch.Size([1, 64, 112, 112])\n",
            "ReLU output shape:\t torch.Size([1, 64, 112, 112])\n",
            "MaxPool2d output shape:\t torch.Size([1, 64, 56, 56])\n",
            "Sequential output shape:\t torch.Size([1, 64, 56, 56])\n",
            "Sequential output shape:\t torch.Size([1, 128, 28, 28])\n",
            "Sequential output shape:\t torch.Size([1, 256, 14, 14])\n",
            "Sequential output shape:\t torch.Size([1, 512, 7, 7])\n",
            "AdaptiveAvgPool2d output shape:\t torch.Size([1, 512, 1, 1])\n",
            "Flatten output shape:\t torch.Size([1, 512])\n",
            "Linear output shape:\t torch.Size([1, 10])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjNdiuhe7INK"
      },
      "source": [
        "## [**Training**]\n",
        "\n",
        "Chúng ta sẽ train với tập MNIST, các bước như các bài notebook trước nhé bạn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCSRQnXZ7NeV"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdjNL6IzNfE0"
      },
      "source": [
        "epochs = 3\n",
        "\n",
        "# Các tham số cần thiết cho quá trình traning.\n",
        "learning_rate = 0.01\n",
        "batch_size = 32\n",
        "display_step = 10\n",
        "\n",
        "# Path lưu best model \n",
        "checkpoint = 'resnet_model.pth' # có thể để dạng *.pth\n",
        "\n",
        "# device chúng ta dùng cuda\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "assert device == 'cuda' "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7Q2z7WRNfSF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a75202e-9a77-4b09-b1e8-bdb1793d1124"
      },
      "source": [
        "# Transform image \n",
        "transform=transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,)) \n",
        "    ])\n",
        "\n",
        "# load dataset từ torchvision.datasets\n",
        "train_dataset = datasets.MNIST('../data', train=True, download=True,transform=transform)\n",
        "test_dataset = datasets.MNIST('../data', train=False,transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=batch_size)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
            "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXNxVqPrNfVV",
        "outputId": "a442dde8-e848-4d25-8aa4-70eafab28974"
      },
      "source": [
        "# call model, set deivce\n",
        "model = net\n",
        "model.to(device)\n",
        "# load lại pretrained model (nếu có)\n",
        "try:\n",
        "  model = torch.load(checkpoint)\n",
        "except:\n",
        "  print(\"!!! Hãy train để có checkpoint file\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!! Hãy train để có checkpoint file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5islDdLNfaE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6a1861b-656a-4e9c-dfd3-6abf957a41f2"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(),lr = learning_rate)\n",
        "best_val_loss = 999\n",
        "\n",
        "for epoch in range(1,epochs):\n",
        "    # Quá trình training \n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device) # device?\n",
        "        optimizer.zero_grad() # Zero_grad\n",
        "        output = model(data)\n",
        "        loss = criterion(output,target)\n",
        "        loss.backward() # backward\n",
        "        optimizer.step() # update weights\n",
        "        if batch_idx % display_step == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "    # Quá trình testing \n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    # set no grad cho quá trình testing\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            output = F.log_softmax(output,1) # log softmax using F, chu y dim nhe\n",
        "            test_loss += loss.item()\n",
        "            pred = torch.argmax(output,1,keepdim=True) # argmax để lấy predicted label, chú ý keepdim = True\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    test_loss /= len(test_loader.dataset) \n",
        "    if test_loss < best_val_loss:\n",
        "      best_val_loss = test_loss\n",
        "      torch.save(model, checkpoint)  # lưu model\n",
        "      print(\"***********    TEST_ACC = {}%    ***********\".format(correct))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tTrain Loss: 2.543706\n",
            "Train Epoch: 1 [320/60000 (1%)]\tTrain Loss: 2.180584\n",
            "Train Epoch: 1 [640/60000 (1%)]\tTrain Loss: 2.212265\n",
            "Train Epoch: 1 [960/60000 (2%)]\tTrain Loss: 1.226463\n",
            "Train Epoch: 1 [1280/60000 (2%)]\tTrain Loss: 1.208656\n",
            "Train Epoch: 1 [1600/60000 (3%)]\tTrain Loss: 1.119853\n",
            "Train Epoch: 1 [1920/60000 (3%)]\tTrain Loss: 0.908041\n",
            "Train Epoch: 1 [2240/60000 (4%)]\tTrain Loss: 0.631488\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tTrain Loss: 0.684465\n",
            "Train Epoch: 1 [2880/60000 (5%)]\tTrain Loss: 0.489946\n",
            "Train Epoch: 1 [3200/60000 (5%)]\tTrain Loss: 0.646968\n",
            "Train Epoch: 1 [3520/60000 (6%)]\tTrain Loss: 0.926786\n",
            "Train Epoch: 1 [3840/60000 (6%)]\tTrain Loss: 0.167033\n",
            "Train Epoch: 1 [4160/60000 (7%)]\tTrain Loss: 0.517705\n",
            "Train Epoch: 1 [4480/60000 (7%)]\tTrain Loss: 0.662639\n",
            "Train Epoch: 1 [4800/60000 (8%)]\tTrain Loss: 0.559478\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tTrain Loss: 0.560070\n",
            "Train Epoch: 1 [5440/60000 (9%)]\tTrain Loss: 0.196011\n",
            "Train Epoch: 1 [5760/60000 (10%)]\tTrain Loss: 0.490352\n",
            "Train Epoch: 1 [6080/60000 (10%)]\tTrain Loss: 0.523570\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tTrain Loss: 0.572436\n",
            "Train Epoch: 1 [6720/60000 (11%)]\tTrain Loss: 0.205201\n",
            "Train Epoch: 1 [7040/60000 (12%)]\tTrain Loss: 0.239229\n",
            "Train Epoch: 1 [7360/60000 (12%)]\tTrain Loss: 0.435000\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tTrain Loss: 0.184686\n",
            "Train Epoch: 1 [8000/60000 (13%)]\tTrain Loss: 1.164487\n",
            "Train Epoch: 1 [8320/60000 (14%)]\tTrain Loss: 0.412642\n",
            "Train Epoch: 1 [8640/60000 (14%)]\tTrain Loss: 0.168742\n",
            "Train Epoch: 1 [8960/60000 (15%)]\tTrain Loss: 0.159179\n",
            "Train Epoch: 1 [9280/60000 (15%)]\tTrain Loss: 0.153606\n",
            "Train Epoch: 1 [9600/60000 (16%)]\tTrain Loss: 0.500047\n",
            "Train Epoch: 1 [9920/60000 (17%)]\tTrain Loss: 0.175416\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tTrain Loss: 0.365293\n",
            "Train Epoch: 1 [10560/60000 (18%)]\tTrain Loss: 0.505820\n",
            "Train Epoch: 1 [10880/60000 (18%)]\tTrain Loss: 0.317052\n",
            "Train Epoch: 1 [11200/60000 (19%)]\tTrain Loss: 0.388168\n",
            "Train Epoch: 1 [11520/60000 (19%)]\tTrain Loss: 0.346899\n",
            "Train Epoch: 1 [11840/60000 (20%)]\tTrain Loss: 0.171843\n",
            "Train Epoch: 1 [12160/60000 (20%)]\tTrain Loss: 0.307664\n",
            "Train Epoch: 1 [12480/60000 (21%)]\tTrain Loss: 0.265577\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tTrain Loss: 0.222740\n",
            "Train Epoch: 1 [13120/60000 (22%)]\tTrain Loss: 0.586343\n",
            "Train Epoch: 1 [13440/60000 (22%)]\tTrain Loss: 0.085935\n",
            "Train Epoch: 1 [13760/60000 (23%)]\tTrain Loss: 0.213630\n",
            "Train Epoch: 1 [14080/60000 (23%)]\tTrain Loss: 0.096275\n",
            "Train Epoch: 1 [14400/60000 (24%)]\tTrain Loss: 0.110180\n",
            "Train Epoch: 1 [14720/60000 (25%)]\tTrain Loss: 0.395181\n",
            "Train Epoch: 1 [15040/60000 (25%)]\tTrain Loss: 0.287130\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tTrain Loss: 0.151171\n",
            "Train Epoch: 1 [15680/60000 (26%)]\tTrain Loss: 0.017595\n",
            "Train Epoch: 1 [16000/60000 (27%)]\tTrain Loss: 0.249103\n",
            "Train Epoch: 1 [16320/60000 (27%)]\tTrain Loss: 0.038375\n",
            "Train Epoch: 1 [16640/60000 (28%)]\tTrain Loss: 0.156325\n",
            "Train Epoch: 1 [16960/60000 (28%)]\tTrain Loss: 0.235352\n",
            "Train Epoch: 1 [17280/60000 (29%)]\tTrain Loss: 0.241725\n",
            "Train Epoch: 1 [17600/60000 (29%)]\tTrain Loss: 0.072962\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tTrain Loss: 0.068514\n",
            "Train Epoch: 1 [18240/60000 (30%)]\tTrain Loss: 0.097184\n",
            "Train Epoch: 1 [18560/60000 (31%)]\tTrain Loss: 0.016368\n",
            "Train Epoch: 1 [18880/60000 (31%)]\tTrain Loss: 0.147835\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tTrain Loss: 0.042127\n",
            "Train Epoch: 1 [19520/60000 (33%)]\tTrain Loss: 0.048995\n",
            "Train Epoch: 1 [19840/60000 (33%)]\tTrain Loss: 0.150364\n",
            "Train Epoch: 1 [20160/60000 (34%)]\tTrain Loss: 0.614839\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tTrain Loss: 0.088141\n",
            "Train Epoch: 1 [20800/60000 (35%)]\tTrain Loss: 0.038418\n",
            "Train Epoch: 1 [21120/60000 (35%)]\tTrain Loss: 0.191222\n",
            "Train Epoch: 1 [21440/60000 (36%)]\tTrain Loss: 0.319276\n",
            "Train Epoch: 1 [21760/60000 (36%)]\tTrain Loss: 0.057021\n",
            "Train Epoch: 1 [22080/60000 (37%)]\tTrain Loss: 0.292975\n",
            "Train Epoch: 1 [22400/60000 (37%)]\tTrain Loss: 0.140576\n",
            "Train Epoch: 1 [22720/60000 (38%)]\tTrain Loss: 0.413794\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tTrain Loss: 0.039374\n",
            "Train Epoch: 1 [23360/60000 (39%)]\tTrain Loss: 0.115543\n",
            "Train Epoch: 1 [23680/60000 (39%)]\tTrain Loss: 0.198022\n",
            "Train Epoch: 1 [24000/60000 (40%)]\tTrain Loss: 0.135608\n",
            "Train Epoch: 1 [24320/60000 (41%)]\tTrain Loss: 0.049122\n",
            "Train Epoch: 1 [24640/60000 (41%)]\tTrain Loss: 0.218486\n",
            "Train Epoch: 1 [24960/60000 (42%)]\tTrain Loss: 0.361647\n",
            "Train Epoch: 1 [25280/60000 (42%)]\tTrain Loss: 0.387289\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tTrain Loss: 0.076492\n",
            "Train Epoch: 1 [25920/60000 (43%)]\tTrain Loss: 0.101199\n",
            "Train Epoch: 1 [26240/60000 (44%)]\tTrain Loss: 0.028198\n",
            "Train Epoch: 1 [26560/60000 (44%)]\tTrain Loss: 0.491091\n",
            "Train Epoch: 1 [26880/60000 (45%)]\tTrain Loss: 0.323504\n",
            "Train Epoch: 1 [27200/60000 (45%)]\tTrain Loss: 0.114775\n",
            "Train Epoch: 1 [27520/60000 (46%)]\tTrain Loss: 0.136374\n",
            "Train Epoch: 1 [27840/60000 (46%)]\tTrain Loss: 0.153968\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tTrain Loss: 0.056732\n",
            "Train Epoch: 1 [28480/60000 (47%)]\tTrain Loss: 0.277542\n",
            "Train Epoch: 1 [28800/60000 (48%)]\tTrain Loss: 0.281084\n",
            "Train Epoch: 1 [29120/60000 (49%)]\tTrain Loss: 0.755002\n",
            "Train Epoch: 1 [29440/60000 (49%)]\tTrain Loss: 0.379968\n",
            "Train Epoch: 1 [29760/60000 (50%)]\tTrain Loss: 0.259481\n",
            "Train Epoch: 1 [30080/60000 (50%)]\tTrain Loss: 0.085056\n",
            "Train Epoch: 1 [30400/60000 (51%)]\tTrain Loss: 0.145907\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tTrain Loss: 0.322598\n",
            "Train Epoch: 1 [31040/60000 (52%)]\tTrain Loss: 0.083449\n",
            "Train Epoch: 1 [31360/60000 (52%)]\tTrain Loss: 0.312613\n",
            "Train Epoch: 1 [31680/60000 (53%)]\tTrain Loss: 0.313329\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tTrain Loss: 0.245398\n",
            "Train Epoch: 1 [32320/60000 (54%)]\tTrain Loss: 0.237852\n",
            "Train Epoch: 1 [32640/60000 (54%)]\tTrain Loss: 0.107980\n",
            "Train Epoch: 1 [32960/60000 (55%)]\tTrain Loss: 0.008403\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tTrain Loss: 0.260511\n",
            "Train Epoch: 1 [33600/60000 (56%)]\tTrain Loss: 0.049132\n",
            "Train Epoch: 1 [33920/60000 (57%)]\tTrain Loss: 0.061421\n",
            "Train Epoch: 1 [34240/60000 (57%)]\tTrain Loss: 0.026837\n",
            "Train Epoch: 1 [34560/60000 (58%)]\tTrain Loss: 0.067586\n",
            "Train Epoch: 1 [34880/60000 (58%)]\tTrain Loss: 0.178883\n",
            "Train Epoch: 1 [35200/60000 (59%)]\tTrain Loss: 0.402874\n",
            "Train Epoch: 1 [35520/60000 (59%)]\tTrain Loss: 0.146190\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tTrain Loss: 0.221484\n",
            "Train Epoch: 1 [36160/60000 (60%)]\tTrain Loss: 0.035344\n",
            "Train Epoch: 1 [36480/60000 (61%)]\tTrain Loss: 0.055623\n",
            "Train Epoch: 1 [36800/60000 (61%)]\tTrain Loss: 0.089785\n",
            "Train Epoch: 1 [37120/60000 (62%)]\tTrain Loss: 0.091939\n",
            "Train Epoch: 1 [37440/60000 (62%)]\tTrain Loss: 0.230990\n",
            "Train Epoch: 1 [37760/60000 (63%)]\tTrain Loss: 0.042233\n",
            "Train Epoch: 1 [38080/60000 (63%)]\tTrain Loss: 0.062285\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tTrain Loss: 0.195768\n",
            "Train Epoch: 1 [38720/60000 (65%)]\tTrain Loss: 0.045738\n",
            "Train Epoch: 1 [39040/60000 (65%)]\tTrain Loss: 0.151129\n",
            "Train Epoch: 1 [39360/60000 (66%)]\tTrain Loss: 0.399313\n",
            "Train Epoch: 1 [39680/60000 (66%)]\tTrain Loss: 0.131836\n",
            "Train Epoch: 1 [40000/60000 (67%)]\tTrain Loss: 0.134949\n",
            "Train Epoch: 1 [40320/60000 (67%)]\tTrain Loss: 0.014418\n",
            "Train Epoch: 1 [40640/60000 (68%)]\tTrain Loss: 0.314557\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tTrain Loss: 0.135803\n",
            "Train Epoch: 1 [41280/60000 (69%)]\tTrain Loss: 0.465521\n",
            "Train Epoch: 1 [41600/60000 (69%)]\tTrain Loss: 0.565012\n",
            "Train Epoch: 1 [41920/60000 (70%)]\tTrain Loss: 0.150997\n",
            "Train Epoch: 1 [42240/60000 (70%)]\tTrain Loss: 0.015071\n",
            "Train Epoch: 1 [42560/60000 (71%)]\tTrain Loss: 0.182461\n",
            "Train Epoch: 1 [42880/60000 (71%)]\tTrain Loss: 0.067639\n",
            "Train Epoch: 1 [43200/60000 (72%)]\tTrain Loss: 0.103196\n",
            "Train Epoch: 1 [43520/60000 (73%)]\tTrain Loss: 0.219383\n",
            "Train Epoch: 1 [43840/60000 (73%)]\tTrain Loss: 0.077392\n",
            "Train Epoch: 1 [44160/60000 (74%)]\tTrain Loss: 0.115935\n",
            "Train Epoch: 1 [44480/60000 (74%)]\tTrain Loss: 0.212149\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tTrain Loss: 0.221768\n",
            "Train Epoch: 1 [45120/60000 (75%)]\tTrain Loss: 0.066433\n",
            "Train Epoch: 1 [45440/60000 (76%)]\tTrain Loss: 0.233473\n",
            "Train Epoch: 1 [45760/60000 (76%)]\tTrain Loss: 0.386739\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tTrain Loss: 0.179299\n",
            "Train Epoch: 1 [46400/60000 (77%)]\tTrain Loss: 0.094707\n",
            "Train Epoch: 1 [46720/60000 (78%)]\tTrain Loss: 0.374704\n",
            "Train Epoch: 1 [47040/60000 (78%)]\tTrain Loss: 0.094207\n",
            "Train Epoch: 1 [47360/60000 (79%)]\tTrain Loss: 0.125956\n",
            "Train Epoch: 1 [47680/60000 (79%)]\tTrain Loss: 0.260195\n",
            "Train Epoch: 1 [48000/60000 (80%)]\tTrain Loss: 0.289773\n",
            "Train Epoch: 1 [48320/60000 (81%)]\tTrain Loss: 0.096914\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tTrain Loss: 0.015312\n",
            "Train Epoch: 1 [48960/60000 (82%)]\tTrain Loss: 0.284247\n",
            "Train Epoch: 1 [49280/60000 (82%)]\tTrain Loss: 0.065697\n",
            "Train Epoch: 1 [49600/60000 (83%)]\tTrain Loss: 0.056031\n",
            "Train Epoch: 1 [49920/60000 (83%)]\tTrain Loss: 0.078404\n",
            "Train Epoch: 1 [50240/60000 (84%)]\tTrain Loss: 0.012852\n",
            "Train Epoch: 1 [50560/60000 (84%)]\tTrain Loss: 0.170875\n",
            "Train Epoch: 1 [50880/60000 (85%)]\tTrain Loss: 0.163778\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tTrain Loss: 0.069118\n",
            "Train Epoch: 1 [51520/60000 (86%)]\tTrain Loss: 0.094322\n",
            "Train Epoch: 1 [51840/60000 (86%)]\tTrain Loss: 0.072691\n",
            "Train Epoch: 1 [52160/60000 (87%)]\tTrain Loss: 0.164968\n",
            "Train Epoch: 1 [52480/60000 (87%)]\tTrain Loss: 0.034031\n",
            "Train Epoch: 1 [52800/60000 (88%)]\tTrain Loss: 0.120590\n",
            "Train Epoch: 1 [53120/60000 (89%)]\tTrain Loss: 0.009881\n",
            "Train Epoch: 1 [53440/60000 (89%)]\tTrain Loss: 0.059832\n",
            "Train Epoch: 1 [53760/60000 (90%)]\tTrain Loss: 0.041041\n",
            "Train Epoch: 1 [54080/60000 (90%)]\tTrain Loss: 0.078004\n",
            "Train Epoch: 1 [54400/60000 (91%)]\tTrain Loss: 0.014240\n",
            "Train Epoch: 1 [54720/60000 (91%)]\tTrain Loss: 0.006968\n",
            "Train Epoch: 1 [55040/60000 (92%)]\tTrain Loss: 0.069799\n",
            "Train Epoch: 1 [55360/60000 (92%)]\tTrain Loss: 0.106050\n",
            "Train Epoch: 1 [55680/60000 (93%)]\tTrain Loss: 0.009615\n",
            "Train Epoch: 1 [56000/60000 (93%)]\tTrain Loss: 0.092509\n",
            "Train Epoch: 1 [56320/60000 (94%)]\tTrain Loss: 0.148221\n",
            "Train Epoch: 1 [56640/60000 (94%)]\tTrain Loss: 0.096742\n",
            "Train Epoch: 1 [56960/60000 (95%)]\tTrain Loss: 0.048461\n",
            "Train Epoch: 1 [57280/60000 (95%)]\tTrain Loss: 0.205455\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tTrain Loss: 0.186541\n",
            "Train Epoch: 1 [57920/60000 (97%)]\tTrain Loss: 0.074309\n",
            "Train Epoch: 1 [58240/60000 (97%)]\tTrain Loss: 0.060654\n",
            "Train Epoch: 1 [58560/60000 (98%)]\tTrain Loss: 0.042812\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tTrain Loss: 0.011842\n",
            "Train Epoch: 1 [59200/60000 (99%)]\tTrain Loss: 0.001250\n",
            "Train Epoch: 1 [59520/60000 (99%)]\tTrain Loss: 0.002270\n",
            "Train Epoch: 1 [59840/60000 (100%)]\tTrain Loss: 0.008780\n",
            "***********    TEST_ACC = 9686%    ***********\n",
            "Train Epoch: 2 [0/60000 (0%)]\tTrain Loss: 0.050434\n",
            "Train Epoch: 2 [320/60000 (1%)]\tTrain Loss: 0.037255\n",
            "Train Epoch: 2 [640/60000 (1%)]\tTrain Loss: 0.046785\n",
            "Train Epoch: 2 [960/60000 (2%)]\tTrain Loss: 0.378082\n",
            "Train Epoch: 2 [1280/60000 (2%)]\tTrain Loss: 0.068865\n",
            "Train Epoch: 2 [1600/60000 (3%)]\tTrain Loss: 0.494547\n",
            "Train Epoch: 2 [1920/60000 (3%)]\tTrain Loss: 0.189510\n",
            "Train Epoch: 2 [2240/60000 (4%)]\tTrain Loss: 0.070745\n",
            "Train Epoch: 2 [2560/60000 (4%)]\tTrain Loss: 0.005062\n",
            "Train Epoch: 2 [2880/60000 (5%)]\tTrain Loss: 0.017448\n",
            "Train Epoch: 2 [3200/60000 (5%)]\tTrain Loss: 0.063733\n",
            "Train Epoch: 2 [3520/60000 (6%)]\tTrain Loss: 0.173484\n",
            "Train Epoch: 2 [3840/60000 (6%)]\tTrain Loss: 0.033949\n",
            "Train Epoch: 2 [4160/60000 (7%)]\tTrain Loss: 0.054207\n",
            "Train Epoch: 2 [4480/60000 (7%)]\tTrain Loss: 0.236592\n",
            "Train Epoch: 2 [4800/60000 (8%)]\tTrain Loss: 0.095217\n",
            "Train Epoch: 2 [5120/60000 (9%)]\tTrain Loss: 0.065531\n",
            "Train Epoch: 2 [5440/60000 (9%)]\tTrain Loss: 0.007605\n",
            "Train Epoch: 2 [5760/60000 (10%)]\tTrain Loss: 0.129864\n",
            "Train Epoch: 2 [6080/60000 (10%)]\tTrain Loss: 0.067496\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tTrain Loss: 0.194951\n",
            "Train Epoch: 2 [6720/60000 (11%)]\tTrain Loss: 0.030831\n",
            "Train Epoch: 2 [7040/60000 (12%)]\tTrain Loss: 0.168069\n",
            "Train Epoch: 2 [7360/60000 (12%)]\tTrain Loss: 0.037473\n",
            "Train Epoch: 2 [7680/60000 (13%)]\tTrain Loss: 0.051483\n",
            "Train Epoch: 2 [8000/60000 (13%)]\tTrain Loss: 0.036499\n",
            "Train Epoch: 2 [8320/60000 (14%)]\tTrain Loss: 0.074170\n",
            "Train Epoch: 2 [8640/60000 (14%)]\tTrain Loss: 0.263344\n",
            "Train Epoch: 2 [8960/60000 (15%)]\tTrain Loss: 0.076791\n",
            "Train Epoch: 2 [9280/60000 (15%)]\tTrain Loss: 0.212276\n",
            "Train Epoch: 2 [9600/60000 (16%)]\tTrain Loss: 0.096137\n",
            "Train Epoch: 2 [9920/60000 (17%)]\tTrain Loss: 0.081763\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tTrain Loss: 0.365585\n",
            "Train Epoch: 2 [10560/60000 (18%)]\tTrain Loss: 0.056486\n",
            "Train Epoch: 2 [10880/60000 (18%)]\tTrain Loss: 0.047485\n",
            "Train Epoch: 2 [11200/60000 (19%)]\tTrain Loss: 0.042783\n",
            "Train Epoch: 2 [11520/60000 (19%)]\tTrain Loss: 0.128079\n",
            "Train Epoch: 2 [11840/60000 (20%)]\tTrain Loss: 0.027284\n",
            "Train Epoch: 2 [12160/60000 (20%)]\tTrain Loss: 0.053019\n",
            "Train Epoch: 2 [12480/60000 (21%)]\tTrain Loss: 0.026946\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tTrain Loss: 0.021939\n",
            "Train Epoch: 2 [13120/60000 (22%)]\tTrain Loss: 0.250024\n",
            "Train Epoch: 2 [13440/60000 (22%)]\tTrain Loss: 0.093519\n",
            "Train Epoch: 2 [13760/60000 (23%)]\tTrain Loss: 0.041295\n",
            "Train Epoch: 2 [14080/60000 (23%)]\tTrain Loss: 0.056800\n",
            "Train Epoch: 2 [14400/60000 (24%)]\tTrain Loss: 0.066955\n",
            "Train Epoch: 2 [14720/60000 (25%)]\tTrain Loss: 0.200084\n",
            "Train Epoch: 2 [15040/60000 (25%)]\tTrain Loss: 0.108470\n",
            "Train Epoch: 2 [15360/60000 (26%)]\tTrain Loss: 0.063002\n",
            "Train Epoch: 2 [15680/60000 (26%)]\tTrain Loss: 0.033587\n",
            "Train Epoch: 2 [16000/60000 (27%)]\tTrain Loss: 0.021457\n",
            "Train Epoch: 2 [16320/60000 (27%)]\tTrain Loss: 0.011306\n",
            "Train Epoch: 2 [16640/60000 (28%)]\tTrain Loss: 0.090516\n",
            "Train Epoch: 2 [16960/60000 (28%)]\tTrain Loss: 0.223828\n",
            "Train Epoch: 2 [17280/60000 (29%)]\tTrain Loss: 0.014213\n",
            "Train Epoch: 2 [17600/60000 (29%)]\tTrain Loss: 0.003775\n",
            "Train Epoch: 2 [17920/60000 (30%)]\tTrain Loss: 0.008938\n",
            "Train Epoch: 2 [18240/60000 (30%)]\tTrain Loss: 0.041757\n",
            "Train Epoch: 2 [18560/60000 (31%)]\tTrain Loss: 0.049670\n",
            "Train Epoch: 2 [18880/60000 (31%)]\tTrain Loss: 0.012527\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tTrain Loss: 0.169259\n",
            "Train Epoch: 2 [19520/60000 (33%)]\tTrain Loss: 0.167055\n",
            "Train Epoch: 2 [19840/60000 (33%)]\tTrain Loss: 0.187425\n",
            "Train Epoch: 2 [20160/60000 (34%)]\tTrain Loss: 0.374597\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tTrain Loss: 0.079949\n",
            "Train Epoch: 2 [20800/60000 (35%)]\tTrain Loss: 0.007057\n",
            "Train Epoch: 2 [21120/60000 (35%)]\tTrain Loss: 0.161704\n",
            "Train Epoch: 2 [21440/60000 (36%)]\tTrain Loss: 0.046989\n",
            "Train Epoch: 2 [21760/60000 (36%)]\tTrain Loss: 0.070404\n",
            "Train Epoch: 2 [22080/60000 (37%)]\tTrain Loss: 0.028082\n",
            "Train Epoch: 2 [22400/60000 (37%)]\tTrain Loss: 0.008840\n",
            "Train Epoch: 2 [22720/60000 (38%)]\tTrain Loss: 0.097344\n",
            "Train Epoch: 2 [23040/60000 (38%)]\tTrain Loss: 0.005062\n",
            "Train Epoch: 2 [23360/60000 (39%)]\tTrain Loss: 0.035210\n",
            "Train Epoch: 2 [23680/60000 (39%)]\tTrain Loss: 0.193846\n",
            "Train Epoch: 2 [24000/60000 (40%)]\tTrain Loss: 0.025380\n",
            "Train Epoch: 2 [24320/60000 (41%)]\tTrain Loss: 0.008237\n",
            "Train Epoch: 2 [24640/60000 (41%)]\tTrain Loss: 0.032590\n",
            "Train Epoch: 2 [24960/60000 (42%)]\tTrain Loss: 0.028282\n",
            "Train Epoch: 2 [25280/60000 (42%)]\tTrain Loss: 0.115982\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tTrain Loss: 0.044638\n",
            "Train Epoch: 2 [25920/60000 (43%)]\tTrain Loss: 0.009485\n",
            "Train Epoch: 2 [26240/60000 (44%)]\tTrain Loss: 0.024626\n",
            "Train Epoch: 2 [26560/60000 (44%)]\tTrain Loss: 0.308927\n",
            "Train Epoch: 2 [26880/60000 (45%)]\tTrain Loss: 0.195020\n",
            "Train Epoch: 2 [27200/60000 (45%)]\tTrain Loss: 0.042612\n",
            "Train Epoch: 2 [27520/60000 (46%)]\tTrain Loss: 0.090558\n",
            "Train Epoch: 2 [27840/60000 (46%)]\tTrain Loss: 0.166816\n",
            "Train Epoch: 2 [28160/60000 (47%)]\tTrain Loss: 0.115559\n",
            "Train Epoch: 2 [28480/60000 (47%)]\tTrain Loss: 0.120118\n",
            "Train Epoch: 2 [28800/60000 (48%)]\tTrain Loss: 0.092047\n",
            "Train Epoch: 2 [29120/60000 (49%)]\tTrain Loss: 0.159319\n",
            "Train Epoch: 2 [29440/60000 (49%)]\tTrain Loss: 0.228186\n",
            "Train Epoch: 2 [29760/60000 (50%)]\tTrain Loss: 0.033807\n",
            "Train Epoch: 2 [30080/60000 (50%)]\tTrain Loss: 0.015670\n",
            "Train Epoch: 2 [30400/60000 (51%)]\tTrain Loss: 0.052047\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tTrain Loss: 0.089838\n",
            "Train Epoch: 2 [31040/60000 (52%)]\tTrain Loss: 0.067449\n",
            "Train Epoch: 2 [31360/60000 (52%)]\tTrain Loss: 0.181842\n",
            "Train Epoch: 2 [31680/60000 (53%)]\tTrain Loss: 0.101633\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tTrain Loss: 0.298609\n",
            "Train Epoch: 2 [32320/60000 (54%)]\tTrain Loss: 0.170317\n",
            "Train Epoch: 2 [32640/60000 (54%)]\tTrain Loss: 0.038220\n",
            "Train Epoch: 2 [32960/60000 (55%)]\tTrain Loss: 0.008311\n",
            "Train Epoch: 2 [33280/60000 (55%)]\tTrain Loss: 0.158495\n",
            "Train Epoch: 2 [33600/60000 (56%)]\tTrain Loss: 0.058818\n",
            "Train Epoch: 2 [33920/60000 (57%)]\tTrain Loss: 0.056291\n",
            "Train Epoch: 2 [34240/60000 (57%)]\tTrain Loss: 0.007631\n",
            "Train Epoch: 2 [34560/60000 (58%)]\tTrain Loss: 0.009965\n",
            "Train Epoch: 2 [34880/60000 (58%)]\tTrain Loss: 0.052795\n",
            "Train Epoch: 2 [35200/60000 (59%)]\tTrain Loss: 0.223636\n",
            "Train Epoch: 2 [35520/60000 (59%)]\tTrain Loss: 0.014387\n",
            "Train Epoch: 2 [35840/60000 (60%)]\tTrain Loss: 0.205988\n",
            "Train Epoch: 2 [36160/60000 (60%)]\tTrain Loss: 0.047874\n",
            "Train Epoch: 2 [36480/60000 (61%)]\tTrain Loss: 0.017177\n",
            "Train Epoch: 2 [36800/60000 (61%)]\tTrain Loss: 0.015492\n",
            "Train Epoch: 2 [37120/60000 (62%)]\tTrain Loss: 0.021149\n",
            "Train Epoch: 2 [37440/60000 (62%)]\tTrain Loss: 0.071440\n",
            "Train Epoch: 2 [37760/60000 (63%)]\tTrain Loss: 0.148608\n",
            "Train Epoch: 2 [38080/60000 (63%)]\tTrain Loss: 0.010837\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tTrain Loss: 0.140262\n",
            "Train Epoch: 2 [38720/60000 (65%)]\tTrain Loss: 0.036983\n",
            "Train Epoch: 2 [39040/60000 (65%)]\tTrain Loss: 0.031864\n",
            "Train Epoch: 2 [39360/60000 (66%)]\tTrain Loss: 0.189544\n",
            "Train Epoch: 2 [39680/60000 (66%)]\tTrain Loss: 0.432288\n",
            "Train Epoch: 2 [40000/60000 (67%)]\tTrain Loss: 0.050820\n",
            "Train Epoch: 2 [40320/60000 (67%)]\tTrain Loss: 0.002167\n",
            "Train Epoch: 2 [40640/60000 (68%)]\tTrain Loss: 0.243822\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tTrain Loss: 0.373768\n",
            "Train Epoch: 2 [41280/60000 (69%)]\tTrain Loss: 0.176814\n",
            "Train Epoch: 2 [41600/60000 (69%)]\tTrain Loss: 0.073611\n",
            "Train Epoch: 2 [41920/60000 (70%)]\tTrain Loss: 0.263989\n",
            "Train Epoch: 2 [42240/60000 (70%)]\tTrain Loss: 0.016480\n",
            "Train Epoch: 2 [42560/60000 (71%)]\tTrain Loss: 0.120680\n",
            "Train Epoch: 2 [42880/60000 (71%)]\tTrain Loss: 0.026291\n",
            "Train Epoch: 2 [43200/60000 (72%)]\tTrain Loss: 0.320449\n",
            "Train Epoch: 2 [43520/60000 (73%)]\tTrain Loss: 0.264465\n",
            "Train Epoch: 2 [43840/60000 (73%)]\tTrain Loss: 0.021938\n",
            "Train Epoch: 2 [44160/60000 (74%)]\tTrain Loss: 0.033629\n",
            "Train Epoch: 2 [44480/60000 (74%)]\tTrain Loss: 0.107432\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tTrain Loss: 0.213369\n",
            "Train Epoch: 2 [45120/60000 (75%)]\tTrain Loss: 0.033200\n",
            "Train Epoch: 2 [45440/60000 (76%)]\tTrain Loss: 0.101309\n",
            "Train Epoch: 2 [45760/60000 (76%)]\tTrain Loss: 0.084604\n",
            "Train Epoch: 2 [46080/60000 (77%)]\tTrain Loss: 0.187955\n",
            "Train Epoch: 2 [46400/60000 (77%)]\tTrain Loss: 0.035619\n",
            "Train Epoch: 2 [46720/60000 (78%)]\tTrain Loss: 0.151264\n",
            "Train Epoch: 2 [47040/60000 (78%)]\tTrain Loss: 0.002458\n",
            "Train Epoch: 2 [47360/60000 (79%)]\tTrain Loss: 0.072048\n",
            "Train Epoch: 2 [47680/60000 (79%)]\tTrain Loss: 0.300255\n",
            "Train Epoch: 2 [48000/60000 (80%)]\tTrain Loss: 0.133438\n",
            "Train Epoch: 2 [48320/60000 (81%)]\tTrain Loss: 0.076022\n",
            "Train Epoch: 2 [48640/60000 (81%)]\tTrain Loss: 0.001387\n",
            "Train Epoch: 2 [48960/60000 (82%)]\tTrain Loss: 0.598051\n",
            "Train Epoch: 2 [49280/60000 (82%)]\tTrain Loss: 0.041453\n",
            "Train Epoch: 2 [49600/60000 (83%)]\tTrain Loss: 0.031619\n",
            "Train Epoch: 2 [49920/60000 (83%)]\tTrain Loss: 0.022294\n",
            "Train Epoch: 2 [50240/60000 (84%)]\tTrain Loss: 0.023575\n",
            "Train Epoch: 2 [50560/60000 (84%)]\tTrain Loss: 0.194163\n",
            "Train Epoch: 2 [50880/60000 (85%)]\tTrain Loss: 0.104239\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tTrain Loss: 0.149874\n",
            "Train Epoch: 2 [51520/60000 (86%)]\tTrain Loss: 0.126949\n",
            "Train Epoch: 2 [51840/60000 (86%)]\tTrain Loss: 0.011523\n",
            "Train Epoch: 2 [52160/60000 (87%)]\tTrain Loss: 0.070428\n",
            "Train Epoch: 2 [52480/60000 (87%)]\tTrain Loss: 0.002634\n",
            "Train Epoch: 2 [52800/60000 (88%)]\tTrain Loss: 0.153190\n",
            "Train Epoch: 2 [53120/60000 (89%)]\tTrain Loss: 0.015352\n",
            "Train Epoch: 2 [53440/60000 (89%)]\tTrain Loss: 0.029993\n",
            "Train Epoch: 2 [53760/60000 (90%)]\tTrain Loss: 0.044503\n",
            "Train Epoch: 2 [54080/60000 (90%)]\tTrain Loss: 0.039957\n",
            "Train Epoch: 2 [54400/60000 (91%)]\tTrain Loss: 0.003325\n",
            "Train Epoch: 2 [54720/60000 (91%)]\tTrain Loss: 0.006350\n",
            "Train Epoch: 2 [55040/60000 (92%)]\tTrain Loss: 0.011508\n",
            "Train Epoch: 2 [55360/60000 (92%)]\tTrain Loss: 0.026204\n",
            "Train Epoch: 2 [55680/60000 (93%)]\tTrain Loss: 0.018445\n",
            "Train Epoch: 2 [56000/60000 (93%)]\tTrain Loss: 0.096013\n",
            "Train Epoch: 2 [56320/60000 (94%)]\tTrain Loss: 0.058363\n",
            "Train Epoch: 2 [56640/60000 (94%)]\tTrain Loss: 0.008063\n",
            "Train Epoch: 2 [56960/60000 (95%)]\tTrain Loss: 0.012751\n",
            "Train Epoch: 2 [57280/60000 (95%)]\tTrain Loss: 0.116258\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tTrain Loss: 0.206581\n",
            "Train Epoch: 2 [57920/60000 (97%)]\tTrain Loss: 0.023713\n",
            "Train Epoch: 2 [58240/60000 (97%)]\tTrain Loss: 0.055715\n",
            "Train Epoch: 2 [58560/60000 (98%)]\tTrain Loss: 0.031761\n",
            "Train Epoch: 2 [58880/60000 (98%)]\tTrain Loss: 0.001586\n",
            "Train Epoch: 2 [59200/60000 (99%)]\tTrain Loss: 0.001324\n",
            "Train Epoch: 2 [59520/60000 (99%)]\tTrain Loss: 0.000278\n",
            "Train Epoch: 2 [59840/60000 (100%)]\tTrain Loss: 0.010675\n",
            "***********    TEST_ACC = 9687%    ***********\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kK2txB5k7QhS"
      },
      "source": [
        "## Tóm tắt\n",
        "\n",
        "* Ánh xạ phần dư có thể học hàm nhận dạng dễ dàng hơn, chẳng hạn như đẩy các tham số trong lớp trọng số về không.\n",
        "* Chúng ta có thể huấn luyện hiệu quả mạng nơ-ron sâu nhờ khối phần dư chuyển dữ liệu liên tầng.\n",
        "* ResNet có ảnh hưởng lớn đến thiết kế sau này của các mạng nơ-ron sâu, cả tích chập và tuần tự.\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}